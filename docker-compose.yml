version: '3'
services:
  zookeeper:
    image: confluentinc/cp-zookeeper
    hostname: kafka-zookeeper
    container_name: kafka-zookeeper
    ports:
      - "2181:2181"
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    networks:
      - my_network
    restart: on-failure:10
    # restart: always
    # chown -R 1000:1000 ./zoo
    volumes: 
      - zoo_data:/var/lib/zookeeper/data:z
      - zoo_log:/var/lib/zookeeper/log:z

  broker:
    image: confluentinc/cp-kafka
    hostname: kafka-broker
    container_name: kafka-broker
    depends_on:
      - zookeeper
    ports:
      - "9092:9092"
      - "29092:29092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: 'kafka-zookeeper:2181'
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: EXTERNAL:PLAINTEXT,INTERNAL:PLAINTEXT
      # if you want to use this image in your local machine :
      #   - add this line to your /etc/hosts file -> 127.0.0.1 kafka-broker  
      # if you want to use this image in a remote server you must do the followings : 
      # 1- change localhost in the following line to the valid IP address of your remote server.
      # 2- add this line to your /etc/hosts file in the remote server -> 127.0.0.1 kafka-broker  
      # AWS : https://rmoff.net/2018/08/02/kafka-listeners-explained/
      KAFKA_ADVERTISED_LISTENERS: EXTERNAL://localhost:909,INTERNAL://kafka-broker:29092
      KAFKA_LISTENERS: EXTERNAL://:9092,INTERNAL://:29092
      KAFKA_INTER_BROKER_LISTENER_NAME: INTERNAL
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
      KAFKA_CONFLUENT_LICENSE_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_CONFLUENT_BALANCER_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_JMX_PORT: 9101
      KAFKA_JMX_HOSTNAME: localhost
      CONFLUENT_METRICS_REPORTER_BOOTSTRAP_SERVERS: kafka-broker:29092
      CONFLUENT_METRICS_REPORTER_TOPIC_REPLICAS: 1
      CONFLUENT_METRICS_ENABLE: 'true'
      CONFLUENT_SUPPORT_CUSTOMER_ID: 'anonymous'

    networks:
      - my_network
    # restart: always
    restart: on-failure:10

    # chown -R 1000:1000 ./broker
    volumes: 
      - kafka_volume:/var/lib/kafka/data:z

  kafkaHQ:
    image: tchiotludo/akhq
    container_name: kafkaHQ
    hostname: kafkaHQ
    depends_on:
      - broker
    ports :
      - 8080:8080
    environment:
      AKHQ_CONFIGURATION: |
        akhq:
          connections:
            docker-kafka-server:
              properties:
                bootstrap.servers: "kafka-broker:29092"
                
    networks:
      - my_network
    restart: on-failure:10
    # restart: always

  namenode:
    image: bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8
    container_name: namenode
    environment:
      - CLUSTER_NAME=test
      - CORE_CONF_fs_defaultFS=hdfs://namenode:9000
      - HDFS_CONF_dfs_namenode_rpc-address=hdfs://namenode:9000
      - HDFS_CONF_dfs_namenode_servicerpc-address=hdfs://namenode:9001
      - HDFS_CONF_dfs_namenode_http-address=hdfs://namenode:50070
      - HDFS_CONF_dfs_namenode_https-address=hdfs://namenode:50470
    ports:
      - "9870:9870" # Expose HDFS UI
    volumes:
      - hadoop_namenode:/hadoop/dfs/name
    networks:
      - my_network

  datanode:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    container_name: datanode
    environment:
      - CORE_CONF_fs_defaultFS=hdfs://namenode:9000
      - HDFS_CONF_dfs_datanode_http-address=datanode:9864
      - HDFS_CONF_dfs_datanode_address=datanode:9866
    volumes:
      - hadoop_datanode:/hadoop/dfs/data
    networks:
      - my_network

  # spark-master:
  #   image: bde2020/spark-master:3.3.0-hadoop3.3
  #   container_name: spark-master
  #   environment:
  #     - SPARK_MODE=master
  #     - SPARK_MASTER_NAME=spark-master
  #     - SPARK_MASTER_PORT=7077
  #     - SPARK_MASTER_WEBUI_PORT=8080
  #   ports:
  #     - "8080:8080"
  #     - "7077:7077"
  #   networks:
  #     - my_network

  # spark-worker:
  #   image: bde2020/spark-worker:3.3.0-hadoop3.3
  #   container_name: spark-worker
  #   environment:
  #     - SPARK_MODE=worker
  #     - SPARK_MASTER=spark://spark-master:7077
  #     - SPARK_WORKER_CORES=2
  #     - SPARK_WORKER_MEMORY=2g
  #     - SPARK_WORKER_PORT=8881
  #     - SPARK_WORKER_WEBUI_PORT=8081
  #   ports:
  #     - "8081:8081"
  #   depends_on:
  #     - spark-master
  #   networks:
  #     - my_network

  get_and_produce_data_to_kafka:
    build:
      context: get_and_produce_data_to_kafka
    container_name: get_and_produce_data_to_kafka
    networks:
      - my_network
    depends_on:
      - broker

  consum_clean_and_save_data_to_hdfs:
    build:
      context: consum_clean_and_save_data_to_hdfs
    container_name: consum_clean_and_save_data_to_hdfs
    networks:
      - my_network
    depends_on:
      - broker
      - namenode


networks:
  my_network:

volumes:
  hadoop_namenode:
  hadoop_datanode:
  kafka_volume:
  zoo_data:
  zoo_log:
