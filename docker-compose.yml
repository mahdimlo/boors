version: '3'
services:
  zookeeper:
    image: confluentinc/cp-zookeeper
    hostname: kafka-zookeeper
    container_name: kafka-zookeeper
    ports:
      - "2181:2181"
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    networks:
      - my_network
    restart: on-failure:10
    # restart: always
    # chown -R 1000:1000 ./zoo
    volumes: 
      - zoo_data:/var/lib/zookeeper/data:z
      - zoo_log:/var/lib/zookeeper/log:z
    healthcheck:
      test: ["CMD", "echo", "ruok | nc localhost 2181 | grep imok"]
      interval: 30s
      timeout: 10s
      retries: 5

  broker:
    image: confluentinc/cp-kafka
    hostname: kafka-broker
    container_name: kafka-broker
    depends_on:
      zookeeper:
        condition: service_healthy
    ports:
      - "9092:9092"
      - "29092:29092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: 'kafka-zookeeper:2181'
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: EXTERNAL:PLAINTEXT,INTERNAL:PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: EXTERNAL://localhost:9092,INTERNAL://kafka-broker:29092
      KAFKA_LISTENERS: EXTERNAL://:9092,INTERNAL://:29092
      KAFKA_INTER_BROKER_LISTENER_NAME: INTERNAL
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
      KAFKA_CONFLUENT_LICENSE_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_CONFLUENT_BALANCER_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_JMX_PORT: 9091
      KAFKA_JMX_HOSTNAME: kafka-broker
      CONFLUENT_METRICS_REPORTER_BOOTSTRAP_SERVERS: kafka-broker:29092
      CONFLUENT_METRICS_REPORTER_TOPIC_REPLICAS: 1
      CONFLUENT_METRICS_ENABLE: 'true'
      CONFLUENT_SUPPORT_CUSTOMER_ID: 'anonymous'

    networks:
      - my_network
    # restart: always
    restart: on-failure:10

    # chown -R 1000:1000 ./broker
    volumes: 
      - kafka_volume:/var/lib/kafka/data:z
    healthcheck:
      test: ["CMD", "nc", "-zv", "localhost", "29092"]
      interval: 30s
      timeout: 10s
      retries: 5

  kafka-ui:
    container_name: kafka-ui
    image: provectuslabs/kafka-ui:latest
    ports:
      - 8080:8080
    depends_on:
      broker:
        condition: service_healthy
    environment:
      KAFKA_CLUSTERS_0_NAME: boors_data
      KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: kafka-broker:29092
      SERVER_SERVLET_CONTEXT_PATH: /kafka-ui
      KAFKA_CLUSTERS_0_METRICS_PORT: 9091
      AUTH_TYPE: "LOGIN_FORM"
      SPRING_SECURITY_USER_NAME: admin
      SPRING_SECURITY_USER_PASSWORD: admin
    networks:
      - my_network
    restart: on-failure:10
    healthcheck:
      test: ["CMD", "nc", "-zv", "localhost", "8080"]
      interval: 30s
      timeout: 10s
      retries: 5

  namenode:
    image: bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8
    container_name: namenode
    environment:
      - CLUSTER_NAME=test
      - CORE_CONF_fs_defaultFS=hdfs://namenode:9000
      - HDFS_CONF_dfs_namenode_rpc-address=hdfs://namenode:9000
      - HDFS_CONF_dfs_namenode_servicerpc-address=hdfs://namenode:9001
      - HDFS_CONF_dfs_namenode_http-address=hdfs://namenode:50070
      - HDFS_CONF_dfs_namenode_https-address=hdfs://namenode:50470
      - HDFS_CONF_dfs_namenode_jmx_port=9868
      - HDFS_NAMENODE_OPTS=-Dcom.sun.management.jmxremote=true
      - HDFS_NAMENODE_OPTS=-Dcom.sun.management.jmxremote.port=9868
      - HDFS_NAMENODE_OPTS=-Dcom.sun.management.jmxremote.authenticate=false
      - HDFS_NAMENODE_OPTS=-Dcom.sun.management.jmxremote.ssl=false
    ports:
      - "9870:9870" # Expose HDFS UI
      - "9868:9868"
    volumes:
      - hadoop_namenode:/hadoop/dfs/name
    networks:
      - my_network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9870"]
      interval: 30s
      timeout: 10s
      retries: 5

  datanode:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    container_name: datanode
    environment:
      - CORE_CONF_fs_defaultFS=hdfs://namenode:9000
      - HDFS_CONF_dfs_datanode_http-address=datanode:9864
      - HDFS_CONF_dfs_datanode_address=datanode:9866
    volumes:
      - hadoop_datanode:/hadoop/dfs/data
    networks:
      - my_network
    depends_on:
      namenode:
        condition: service_healthy

  # spark-master:
  #   image: bde2020/spark-master:3.3.0-hadoop3.3
  #   container_name: spark-master
  #   environment:
  #     - SPARK_MODE=master
  #     - SPARK_MASTER_NAME=spark-master
  #     - SPARK_MASTER_PORT=7077
  #     - SPARK_MASTER_WEBUI_PORT=8080
  #   ports:
  #     - "8080:8080"
  #     - "7077:7077"
  #   networks:
  #     - my_network

  # spark-worker:
  #   image: bde2020/spark-worker:3.3.0-hadoop3.3
  #   container_name: spark-worker
  #   environment:
  #     - SPARK_MODE=worker
  #     - SPARK_MASTER=spark://spark-master:7077
  #     - SPARK_WORKER_CORES=2
  #     - SPARK_WORKER_MEMORY=2g
  #     - SPARK_WORKER_PORT=8881
  #     - SPARK_WORKER_WEBUI_PORT=8081
  #   ports:
  #     - "8081:8081"
  #   depends_on:
  #     - spark-master
  #   networks:
  #     - my_network

  get_and_produce_data_to_kafka:
    build:
      context: get_and_produce_data_to_kafka
    container_name: get_and_produce_data_to_kafka
    networks:
      - my_network
    depends_on:
      broker:
        condition: service_healthy

  consum_clean_and_save_data_to_hdfs:
    build:
      context: consum_clean_and_save_data_to_hdfs
    container_name: consum_clean_and_save_data_to_hdfs
    networks:
      - my_network
    depends_on:
      namenode:
        condition: service_healthy
      broker:
        condition: service_healthy

  nginx:
    build:
      context: nginx
    container_name: nginx
    ports:
      - "80:80"
    environment:
      USERNAME: admin
      PASSWORD: admin
    networks:
      - my_network
    depends_on:
      namenode:
        condition: service_healthy
      kafka-ui:
        condition: service_healthy

networks:
  my_network:

volumes:
  hadoop_namenode:
  hadoop_datanode:
  kafka_volume:
  zoo_data:
  zoo_log:
